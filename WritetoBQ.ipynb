{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d1b00ef-dac4-4e3a-ae6a-c0c49cd4496a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/jupyter/.kernels/apache-beam-2.49.0/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpl81p32at', 'apache-beam==2.49.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/jupyter/.kernels/apache-beam-2.49.0/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpl81p32at', 'apache-beam==2.49.0', '--no-deps', '--only-binary', ':all:', '--python-version', '38', '--implementation', 'cp', '--abi', 'cp38', '--platform', 'manylinux2014_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.49.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Pipeline has additional dependencies to be installed in SDK worker container, consider using the SDK container image pre-building workflow to avoid repetitive installations. Learn more on https://cloud.google.com/dataflow/docs/guides/using-custom-containers#prebuild\n",
      "INFO:root:Using provided Python SDK container image: gcr.io/cloud-dataflow/v1beta3/beam_python3.8_sdk:2.49.0\n",
      "INFO:root:Python SDK container image set to \"gcr.io/cloud-dataflow/v1beta3/beam_python3.8_sdk:2.49.0\" for Docker environment\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://maniprakash-bucket/staging/pubsub-to-bq-batch.1691160193.522591/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://maniprakash-bucket/staging/pubsub-to-bq-batch.1691160193.522591/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://maniprakash-bucket/staging/pubsub-to-bq-batch.1691160193.522591/apache_beam-2.49.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://maniprakash-bucket/staging/pubsub-to-bq-batch.1691160193.522591/apache_beam-2.49.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://maniprakash-bucket/staging/pubsub-to-bq-batch.1691160193.522591/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://maniprakash-bucket/staging/pubsub-to-bq-batch.1691160193.522591/pipeline.pb in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding flag -f, single dash flags are not allowed.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding flag -f, single dash flags are not allowed.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " clientRequestId: '20230804144313523626-9673'\n",
      " createTime: '2023-08-04T14:43:15.509383Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2023-08-04_07_43_15-848919364116847496'\n",
      " location: 'europe-west2'\n",
      " name: 'pubsub-to-bq-batch'\n",
      " projectId: 'my-another-394512'\n",
      " stageStates: []\n",
      " startTime: '2023-08-04T14:43:15.509383Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_STREAMING, 2)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2023-08-04_07_43_15-848919364116847496]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2023-08-04_07_43_15-848919364116847496\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/europe-west2/2023-08-04_07_43_15-848919364116847496?project=my-another-394512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "makjshsgyeadnaskdsakdnlkasn\n",
      "mmmmmmmmmmnnnnnnnbvvvvvvvvvvvvvvvvassssssssssss\n",
      "mmmmmmmmmmnnnnnnnbvvvvvvvvvvvvvvvvassssssssssss\n",
      "mmmmmmmmmmnnnnnnnbvvvvvvvvvvvvvvvvassssssssssss\n",
      "mmmmmmmmmmnnnnnnnbvvvvvvvvvvvvvvvvassssssssssss\n",
      "mnbvcxz\n",
      "mnbvcxz\n",
      "mnbvcxz\n",
      "mnbvcxz\n",
      "mmmmmmmmmmnnnnnnnbvvvvvvvvvvvvvvvvassssssssssss\n",
      "mnbvcxz\n",
      "makjshsgyeadnaskdsakdnlkasn\n",
      "mmmmmmmmmmnnnnnnnbvvvvvvvvvvvvvvvvassssssssssss\n",
      "mnbvcxz\n",
      "mnbvcxz\n",
      "mnbvcxz\n",
      "mnbvcxz\n",
      "mnbvcxz\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.transforms.trigger import AfterWatermark, AfterCount\n",
    "import logging\n",
    "\n",
    "class PrintMessages(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        print(element)\n",
    "        yield element\n",
    "\n",
    "def run():\n",
    "    pipeline_options = PipelineOptions()\n",
    "\n",
    "    # Set the Google Cloud project and specify the Dataflow runner\n",
    "    google_cloud_options = pipeline_options.view_as(GoogleCloudOptions)\n",
    "    google_cloud_options.project = 'my-another-394512'\n",
    "    google_cloud_options.job_name = 'pubsub-to-bq-batch'\n",
    "    google_cloud_options.staging_location = 'gs://maniprakash-bucket/staging'\n",
    "    google_cloud_options.temp_location = 'gs://maniprakash-bucket/temp'\n",
    "    google_cloud_options.region = 'europe-west2'\n",
    "\n",
    "    # Enable streaming mode\n",
    "    pipeline_options.view_as(beam.options.pipeline_options.StandardOptions).streaming = True\n",
    "    pipeline_options.view_as(beam.options.pipeline_options.StandardOptions).runner = 'DataflowRunner'\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipeline = beam.Pipeline(options=pipeline_options)\n",
    "\n",
    "    # Read messages from Pub/Sub topic\n",
    "    messages = (\n",
    "        pipeline\n",
    "        | 'Read from Pub/Sub' >> beam.io.ReadFromPubSub(subscription='projects/my-another-394512/subscriptions/my-topic-sub')\n",
    "        | 'Decode message' >> beam.Map(lambda x: x.decode('utf-8'))\n",
    "       # | 'Add timestamps' >> beam.Map(lambda x: beam.window.TimestampedValue(x, 0))  # Assign fixed timestamps\n",
    "    )\n",
    "\n",
    "    # Apply windowing and triggers\n",
    "    windowed_messages = messages | 'Apply Windowing' >> beam.WindowInto(\n",
    "        beam.window.FixedWindows(60),  # 1-minute window\n",
    "        trigger=AfterWatermark(early=AfterCount(10), late=AfterCount(20)),  # Batch trigger\n",
    "        accumulation_mode=beam.trigger.AccumulationMode.DISCARDING\n",
    "    )\n",
    "\n",
    "    # Print messages to console\n",
    "    windowed_messages | 'Print Messages' >> beam.ParDo(PrintMessages())\n",
    "\n",
    "    # Write messages to BigQuery\n",
    "    table_spec = 'my-another-394512.mydataset.pubsub'\n",
    "    windowed_messages | 'Write to BigQuery' >> beam.io.WriteToBigQuery(\n",
    "        table_spec,\n",
    "        schema='msg:STRING',  # Define your schema here\n",
    "        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED\n",
    "    )\n",
    "\n",
    "    pipeline.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1f5f3-8947-4463-b708-495de3840e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Apache Beam 2.49.0 for Python 3",
   "language": "python",
   "name": "01-apache-beam-2.49.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
